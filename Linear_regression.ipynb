{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression: Finding the Best of the Best (Lines of Fit)\n",
    "In this short notebook, we will demonstrate a simple simple supervised technique for finding trends in data.\n",
    "\n",
    "Regression models in general seek relationships between an independent variable and some number of dependent variables. Notably, only continuous Linear regression models are a subset which assume that said relationship is linear in nature, that is, it is of the form $ y = mx + b $. It is the goal to estimate and optimize parameters $ m $ and $ b $ such that overall error (which can be quantified by one of several methods) is minimized. \n",
    "\n",
    "There are two main types: \n",
    "\n",
    "(1) Simple: entails only one independent variable ($x$). \n",
    "(2) Multiple: entails any number of independent variables greater than one. \n",
    "\n",
    "To start, this tutorial will focus on (2). Let's begin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Little Housekeeping\n",
    "We will first import all dependencies and set up our programming environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reset the workspace, forcibly deleting all existing variables \n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker\n",
    "from matplotlib.dates import DateFormatter, AutoDateFormatter\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Data\n",
    "Linear regression can be performed on any dataset, regardless of whether a clear upward or downward trend can be extrapolated. However, for the sake of demonstration, we will choose a set which does have a clear trend. \n",
    "\n",
    "Thus we'll be using a straightforward set of all-time historical stock data for Merck & Co (MRK). Our set includes years 1970 to 2020, during which time this stock has enjoyed a substantial increase in value. It was first listed on the Down Jones Industrial Average in 1979. Source: Yahoo Finance (https://finance.yahoo.com/quote/MRK/history?p=MRK)\n",
    "\n",
    "We'll first import the data to a Pandas dataframe, preview the first handful of rows, and print a few statistics in order to get a feel for the data. \n",
    "\n",
    "Please note that the file, which is .csv format, will be saved in a directory named 'Data.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/MRK.csv',\n",
    "                parse_dates=['Date'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the data types of each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, note that the type of entries in the 'Date' field are of type datetime64[ns]. This is NumPy's functional equivalent of Python's builtin datetime function. datetime64[ns] encodes dates as 64-bit integers, which are very compact and good for vectorization. [ns] is a format code that specifies precision to the nanosecond. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Dataframe shape:', df.shape)\n",
    "print('Number of data examples:', df.shape[0])\n",
    "print('Number of data features:', df.shape[1])\n",
    "print('Number of dimensions:', df.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it may be time costly to plot almost 13,000 points, we will take every fifth data point and omit all others. The overall trend and result of our regression analysis will end up essentially the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.iloc[::5, :] # Include every fifth row\n",
    "print('Dataframe shape:', df.shape)\n",
    "print('Number of data examples:', df.shape[0])\n",
    "print('Number of data features:', df.shape[1])\n",
    "print('Number of dimensions:', df.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also check for missing values. There are none, as evidenced by the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Presence of Any Missing Value, by Column: \\n')\n",
    "print(df.isnull().any(), '\\n')\n",
    "\n",
    "print('Just to Confirm - Sum of Missing Values, by Column: \\n')\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be useful to display summary statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They say a picture is worth a thousand words. We'll now visualize the data using matplotlib, which can easily pull information from a Pandas dataframe. \n",
    "\n",
    "A small caveat is that the independent (x) and dependent (y) variables must be specified in order to use the df.plot() function. For that, it is trivially easy to simply look at the column headers displayed in df.head(). \n",
    "\n",
    "Another handy way to access these headers is by calling the columns attribute of the dataframe using df.columns, as seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns # Displays column headers needed for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot stock close values from all time (1970 until present) with date as the independent variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: format to match other plots \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8), facecolor = 'white') # Sets plot size and color behind axes\n",
    "ax.set_title('MRK Close Value vs. Date, 1970 - 2020', fontsize=18) # Title\n",
    "date_form = DateFormatter(\"%Y\") # Formats x axis dates\n",
    "ax.xaxis.set_major_formatter(date_form)\n",
    "loc = plticker.MultipleLocator(base=365.0*4) # Set interval / frequency of x axis ticks\n",
    "ax.xaxis.set_major_locator(loc) \n",
    "df.plot(kind = 'scatter', x = 'Date', y = 'Close', ax = ax, s = 1) # Plot data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "### Format 'Date' Column\n",
    "As you can see in the code below, the entries contained in the 'Date' column of the dataframe df are of the type pandas Timestamp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Date entry:', df['Date'][0])\n",
    "print('Date entry type:', type(df['Date'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression methods do not handle this format well, so we will start by converting each of these timestamps into an ordinal value, that is, a number to represent the date and time. \n",
    "\n",
    "This is relatively straightforward to do using the toordinal function within the datetime module. Simply put, this function assigns every day from January 1st of the year 1 to December 31st of the 9,999th year its own number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = pd.to_datetime(df['Date'])\n",
    "x = x.map(dt.datetime.toordinal).to_numpy()\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change from datetime to ordinal value should be apparent in dataframe dtypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything has been nicely converted, we'll define the dependent variable and adjust dimensions to allow for matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = df['Close'].to_numpy()\n",
    "m = len(x)\n",
    "\n",
    "# Add a dimension to x and y to allow array multiplication later\n",
    "x, y = np.expand_dims(x, axis = 1), np.expand_dims(y, axis = 1)\n",
    "\n",
    "print('\\nx.shape:', x.shape, 'type:', type(x))\n",
    "print('y.shape:', y.shape, 'type:', type(y), '\\n')\n",
    "\n",
    "print('x, containing converted datetimes: \\n\\n', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize x values for more efficient convergence \n",
    "After normalization, values will be in the range [0,1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Close values range:', (max(x) - min(x)))\n",
    "x_norm = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, everything looks at it should. Note that the date variable (displayed on the x axis) now displays as an ordinal number rather than as a year. This could be further formatted, but I've left it as-is to demonstrate the point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO : return J as a float \n",
    "\n",
    "def compute_cost(hypothesis, y):\n",
    "    \"\"\"\n",
    "    Calculate the cost function. \n",
    "    \n",
    "    To be run repeatedly as gradient descent executes.\n",
    "\n",
    "    Args:\n",
    "        hypothesis (numpy.ndarray): array of size (m, 1) containing hypothesized target values \n",
    "        y (numpy.ndarray): array of size (m, 1) containing target values\n",
    "\n",
    "    Returns:\n",
    "        J (numpy.float64): cost, or the root mean squared error between hypothesis and target solution (y)\n",
    "        \n",
    "    \"\"\"      \n",
    "    num_samples = len(y)\n",
    "    J = (1/num_samples) * sum((hypothesis - y) ** 2)\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Hypothesis Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_hypothesis(hypothesis, x, weights):\n",
    "    \"\"\"\n",
    "    Plots, and displays the function of, a line. \n",
    "\n",
    "    Args:\n",
    "        hypothesis (numpy.ndarray): array of size (m, 1) containing hypothesized target values \n",
    "        x (numpy.ndarray): array of size (2556, 1) containing x data\n",
    "        weights (numpy.ndarray): array of size (2, 1) for linear regression in two dimensions. \n",
    "            weights[0] is the value of the intercept, b; weights[1] is the slope parameter, m. \n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    \"\"\" \n",
    "    h = ax.plot(x, hypothesis)\n",
    "    \n",
    "    w0 = \"{:.2f}\".format(round(weights[0][0], 2))\n",
    "    w1 = \"{:.2f}\".format(round(weights[1][0], 2))\n",
    "    legend_text = str(w1 + \"x\" + \" + \" + w0)  \n",
    "    \n",
    "    ax.legend(h, ['Hypothesis: ' + legend_text], fontsize='xx-large', loc='upper left')\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Gradient Descent Function\n",
    "Gradient descent is a function that finds the minimum (local, or global - more on that later) of a function by repetitively calculating its gradient, and moving in the direction that it stipulates will lead to the lowest nearby point. There's a lot of calculus here, and to follow this proof requires knowledge of partial derivatives. If you don't know calculus, you can skip this part and move on, but in the long run, these topics are crucial for an in-depth understanding. \n",
    "\n",
    "Please note that m = len(x), or the number of samples in the data set. \n",
    "\n",
    "Cost function: \n",
    "\n",
    "* $ J(w_{0}, w_{1}) = \\frac{1}{2m}\\sum_{m}^{i=1}(h(x^{(i)}) - y^ {(i)}))^{2} $\n",
    "\n",
    "Hypothesis function: \n",
    "\n",
    "* $ h(x) = w_{1}x + w_{0} $\n",
    "\n",
    "Substitute the hypothesis function $h(x) $ into the cost function, $ J $, in order to optimize $ J $ with respect to the parameters contained within $ h(x) $; recall that these are the weights, $ w_0 $ and $ w_1 $: \n",
    "\n",
    "* $ J = \\frac{1}{2m} \\sum_{m}^{i=1} ((w_{1}x + w_{0})) - y^ {(i)})^{2} $\n",
    "\n",
    "\n",
    "Since we are trying to optimize the weights, a.k.a. parameters of the function (which are demarcated by $ w_0 $ and $ w_1 $), we will take the gradient of the cost function with respect to each of these two parameters. \n",
    "\n",
    "Note that the gradient is simply a vector of partial derivates with respect to each component. Thus, \n",
    "\n",
    "* $ \\nabla_{\\mathbf{w_0}} J = \\frac{\\partial }{\\partial w_0} J(w_0, w_1) = \\frac{2}{2m} \\sum((w_1x^{(i)} + w_0) - y^{(i)})\\cdot (1))$\n",
    "\n",
    "* $ \\nabla_{\\mathbf{w_1}} J = \\frac{\\partial }{\\partial w_1} J(w_0, w_1) = \\frac{2}{2m} \\sum((w_1x^{(i)} + w_0) - y^{(i)})\\cdot (x^{(i)}) $\n",
    "\n",
    "Note that the partial derivate of $ J $ with respect to each parameter involves the Chain Rule. \n",
    "\n",
    "Repeat until convergence: \n",
    "\n",
    "* $ w_{0new} := w_{0old} - \\alpha \\frac{\\partial }{\\partial w_0} J(w_0, w_1) $\n",
    "\n",
    "  $ = w_{0old} - \\alpha \\frac{2}{2m} \\sum((w_1x^{(i)} + w_0) - y^{(i)})\\cdot (1) $\n",
    "\n",
    "and \n",
    "\n",
    "\n",
    "* $ w_{1new} := w_{1old} - \\alpha \\frac{\\partial }{\\partial w_1} J(w_0, w_1) $\n",
    "\n",
    "* $ w_{1new} := w_{1old} - \\alpha \\frac{2}{2m} \\sum((w_1x^{(i)} + w_0) - y^{(i)})\\cdot (x^{(i)}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, weights, learning_rate, epochs):\n",
    "    \"\"\"\n",
    "    Iteratively compute and minimize cost by updating weights. \n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): shape (len(x), 2), where X[:,1:2] equals data values\n",
    "            and X[:, 0:1] is a column of ones that makes array dimensions \n",
    "            agreeable for matrix multiplication. \n",
    "        y (numpy.ndarray): size (m, 1) array containing target values \n",
    "        weights (numpy.ndarray): array of size (2, 1) for linear regression in two dimensions. \n",
    "            weights[0] is the value of the intercept, b; weights[1] is the slope parameter, m. \n",
    "        learning_rate (float): dictates 'step size' used for gradient descent. Values that are too small may take \n",
    "            a very long time to converge; very large values may cause gradient descent to diverge\n",
    "        epochs (int): set desired number of iterations. A larger value requires more time to run and may return results\n",
    "            with higher confidence. \n",
    "\n",
    "    Returns:\n",
    "        hypothesis (numpy.ndarray): array of size (m, 1) containing hypothesized target values \n",
    "        weights (numpy.ndarray): array of size (2, 1) containing new weights after update via gradient descent. \n",
    "        cost_history (numpy.ndarray): contains cost J calculated during each epoch. \n",
    "            Array size will thus be (epochs, 1)\n",
    "\n",
    "    \"\"\"   \n",
    "    num_samples = len(y)\n",
    "    cost_history = np.zeros([epochs, 1])\n",
    "\n",
    "    for i in range(epochs):\n",
    "        hypothesis = np.matmul(X, weights)\n",
    "        \n",
    "        # Find gradient of cost function J with respect to each of the two parameters (w1 and w2)\n",
    "        gradient_wrt_weight0 = (1/num_samples) * sum(hypothesis - y) * 1\n",
    "        gradient_wrt_weight1 = (1/num_samples) * sum((hypothesis - y) * X[:,1:2])\n",
    "        \n",
    "        weight0_new = weights[0] - learning_rate * gradient_wrt_weight0\n",
    "        weight1_new = weights[1] - learning_rate * gradient_wrt_weight1\n",
    "        \n",
    "        weights[0] = weight0_new\n",
    "        weights[1] = weight1_new\n",
    "        \n",
    "        # print('W0:', weights[0])\n",
    "        # print('W1:', weights[1])\n",
    "        \n",
    "        cost = compute_cost(hypothesis, y)\n",
    "        cost_history[i] = cost\n",
    "        # print('Cost:', cost)\n",
    "        \n",
    "# TODO - update input from x to a function of X        \n",
    "        plot_hypothesis(hypothesis, x, weights)\n",
    "        \n",
    "        if i != epochs - 1:\n",
    "            del ax.lines[0] # erase line before next iteration, as long as there will be more\n",
    "\n",
    "    return hypothesis, weights, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Program\n",
    "First, choose values for hyperparameters learning rate and number of epochs. There is no definitive rule for choosing these values; experienced practitioners may be able to \"intuit\" those that will converge faster. For others, it can require empirical observation and patience. \n",
    "\n",
    "We will also initialize parameters (weights) using Numpy's builtin random function. \n",
    "\n",
    "We need also to create a new array, X, which contains ones in its first column, and the x data in its second. We do this so that the values in x can be matrix multiplied against those in the weights array without raising a dimensionality disagreement. \n",
    "\n",
    "As a quick aside, this is necessary because, in linear algebra, the 'inner' dimensions of matrices that are multipled must match. \n",
    "\n",
    "For example, the following is valid: [2 x 3]\\*[3 * 1] → [2 x 1] matrix\n",
    "\n",
    "This is not: [2 x 3]\\*[4 x 5], because 3 ≠ 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.007 # Also called 'alpha'\n",
    "epochs = 500 # As desired\n",
    "weights = np.random.rand(2,1) # weights[0] is intercept b; weights[1] is slope m\n",
    "\n",
    "X = np.ones([m, 2])\n",
    "X[:, 1:2] = x_norm\n",
    "\n",
    "print('Y intercept:', weights[0][0])\n",
    "print('Slope:', weights[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.ion()\n",
    "\n",
    "x_margin = (max(x) - min(x))*.10\n",
    "y_margin = (max(y) - min(y))*.10\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8), facecolor = 'white') # Sets plot size and color behind axes\n",
    "ax.set_title('MRK Close Value vs. Date, 1970 - 2020', fontsize = 18)\n",
    "plt.xlabel('Date', fontsize = 14)\n",
    "plt.ylabel('Close Value', fontsize = 14)\n",
    "plt.xlim(min(x) - x_margin, max(x) + x_margin)\n",
    "plt.ylim(min(y) - y_margin, max(y) + y_margin)\n",
    "plt.scatter(x, y, s=1)\n",
    "\n",
    "hypothesis, weights, cost_history = gradient_descent(X, y, weights, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model found the line of best fit. Regression can be a powerful tool for predicting general tendencies over the time, but its usefulness as a machine learning technique is inversely correlated with its adherence (in any particular scenario) to the assumptions of the method itself. For each dataset, be sure to evaluate whether linear regression assumptions are valid. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
