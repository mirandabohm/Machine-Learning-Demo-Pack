{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network (ANN)\n",
    "\n",
    "***Overview***\n",
    "\n",
    "An artificial neural network (ANN) is a computational model inspired by biological networks of neurons found in the brain. It consists of interconnected units (\"neurons\" or \"nodes\") which receive, process, and transmit information. They are capable of recognizing intricate patterns, modeling complex relationships, and making useful predictions. \n",
    "\n",
    "Relationships between nodes are represented by \"weights\" or float values which signify the strength of the connection between them. These are adjusted during the training process as the model learns relevant patterns and attempts to optimize performance. \n",
    "\n",
    "The flow of information through an ANN mimics the flow of electrochemical signals as they propagate through brain tissue. It can be described as follows:  \n",
    "\n",
    "### Layered Structure ###\n",
    "\n",
    "1. Input layer: receives raw data as input\n",
    "2. Hidden layer(s): one or more intermediate layers that perform additional operations\n",
    "3. Output layer: generates a final prediction\n",
    "\n",
    "**The Model**\n",
    "\n",
    "The following describes the information processing that occurs in a single layer:\n",
    "\n",
    "$$\n",
    "O = f(W \\cdot X + b)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- *X*: the node's input as a vector whose dimensionality depends on the number of neurons in the previous layer\n",
    "- *W*: the weight matrix for the layer, whose values update as training occurs \n",
    "- *b*: a vector of \"bias\" values that shift the activation function independently of the input, enabling it to better capture complex patterns and relationships in the data\n",
    "- *f*: the activation function applied to the weighted sum of inputs\n",
    "- *O*: output representing transformed data. \n",
    "\n",
    "**Training Process**\n",
    "\n",
    "During the training process, the network adjusts its weights and biases to minimize the error (or loss function) between its predictions and the actual output. The learning process typically uses **gradient descent** or variants to update the weights.\n",
    "\n",
    "$$\n",
    "\\text{New weight} = \\text{Old weight} - \\eta \\cdot \\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "\n",
    "Where **\\eta** is the learning rate.\n",
    "\n",
    "The gradient of the loss function **L** with respect to the weight matrix **W** is expressed by\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "\n",
    "**Activation Function Selection**\n",
    "\n",
    "An activation function is applied to the weighted sum of inputs plus bias at each layer, introducing nonlinearity to the model. It plays a critical role by determining the output of each neuron based on its input, enabling ANNs to model complex relationships. \n",
    "\n",
    "Examples include: \n",
    "\n",
    "1. Sigmoid: outputs values between 0 and 1. \n",
    "2. Rectified linear unit (ReLU): popular in the hidden layers of deep networks, ReLU \"rectifies\" negative values to 0 while leaving positive values unchanged. \n",
    "3. Softmax: commonly applied in the output layer for multi-class classification problems. Outputs a normalized vector of probabilities between 0 and 1 whose sum equals 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Implementation in Python ***\n",
    "\n",
    "We'll be setting up our model manually to demonstrate its inner mechanics. This approach helps us understand the fundamental concepts behind neural networks, such as how layers, weights, and activation functions work together to make predictions.\n",
    "\n",
    "In practice, it can be convenient to use one of several powerful libraries and frameworks for neural network implementation. These offer pre-built models with various architectures and optimization routines to streamline model development and training by removing the need for writing low-level code from scratch. In this notebook, we will utilize TensorFlow, an open-source framework developed by Google. It provides a comprehensive ecosystem for building and deploying machine learning models. TensorFlow is widely used because it offers:\n",
    "\n",
    "- Ease of Use: High-level APIs like Keras make it easy to build and train models.\n",
    "- Performance: Optimized for both CPU and GPU, allowing for efficient training and inference.\n",
    "- Flexibility: Supports a wide range of machine learning tasks, from simple linear regression to complex deep learning models.\n",
    "- Community and Support: A large community of developers and extensive documentation.\n",
    "- Alternatives to TensorFlow\n",
    "\n",
    "Alternatives include: \n",
    "\n",
    "- PyTorch: Developed by Facebook, PyTorch is known for its dynamic computation graph and ease of use, especially in research settings.\n",
    "- Keras: Initially an independent project, Keras is now part of TensorFlow. It provides a high-level API for building neural networks.\n",
    "- MXNet: An open-source deep learning framework that is scalable and supports multiple languages.\n",
    "- Caffe: Developed by the Berkeley Vision and Learning Center (BVLC), Caffe is known for its speed and modularity.\n",
    "- Theano: One of the earliest deep learning libraries, Theano is now mostly used for educational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
