{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network (ANN)\n",
    "\n",
    "***Overview***\n",
    "\n",
    "<p style=\"float: right; margin: 0 0 10px 10px;\">\n",
    "  <a data-flickr-embed=\"true\" data-header=\"true\" href=\"https://en.wikipedia.org/wiki/Cerebral_cortex#/media/File:Minute_structure_of_the_cerebral_cortex.jpg\" title=\"Caught in the App LONDON\">\n",
    "    <img \n",
    "      src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Minute_structure_of_the_cerebral_cortex.jpg/800px-Minute_structure_of_the_cerebral_cortex.jpg\" \n",
    "      alt=\"Neocortex layers\" \n",
    "      width=\"400\" \n",
    "      height=\"666\">\n",
    "  \n",
    "  </a>\n",
    "</p>\n",
    "\n",
    "An artificial neural network (ANN) is a computational model inspired by biological networks of neurons found in the brain. Consisting of interconnected units (\"neurons\" or \"nodes\") organized in layers, these models are capable of recognizing intricate patterns, modeling complex relationships, and making useful predictions. Generally, a model includes an input layer, wherein data is initially fed; one or more hidden layers, which process and transform the data; and an output layer, which produces the final classification or prediction. Connections between nodes vary in their strength, characterized by a \"weight\" (represented as a float value) and are associated with a \"bias\" or adjustment term. \n",
    "\n",
    "An ANN \"learns\" via an iterative process of weight adjustment (\"training\"). During this process, the model makes predictions, compares these to known values (\"labels\"), and computes the difference (\"error\"). Using a process of optimization, the model attempts to minimize this error by adjusting weights and biases iteratively. \n",
    "\n",
    "_Right: biological neurons spanning vertically-organized layers of neocortex. Source: [Encyclopedia Britannica - 1911 EB, Vol. 4, Page 400](https://en.wikipedia.org/wiki/Cerebral_cortex)_\n",
    "\n",
    "***Layered Structure of an ANN***\n",
    "\n",
    "1. Input layer: receives raw data as input. The number of neurons in the input layer corresponds to the number of features of the dataset. \n",
    "2. Hidden layer(s): one or more intermediate layers that process input data through various mathematical operations. These layers enable the network to learn complex patterns. Generally, the \"deepest\" layer typically refers to the hidden layer furthest from the input layer. \n",
    "3. Output layer: the final layer, responsible for producing the model's prediction. The number of neurons here corresponds to the number of output classes (in the case of a classification task) or a single value (in the case of a regression task).\n",
    "\n",
    "***The Model***\n",
    "\n",
    "Each neuron in an ANN performs a weighted sum of its inputs, adds a bias, and applies an activation function to produce an ouput. This can be expressed mathematically for each layer as: \n",
    "\n",
    "$$\n",
    "O = f(W \\cdot X + b)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $X$: the node's input as a vector whose dimensionality depends on the number of neurons in the previous layer\n",
    "- $W$: the weight matrix for the layer, whose values update as training occurs \n",
    "- $b$: a vector of \"bias\" values corresponding to each neuron in the layer\n",
    "- $f$: the activation function applied to the weighted sum of inputs\n",
    "- $O$: output representing transformed data, and \n",
    "- $W \\cdot X $: the dot product of $W$ and $X$\n",
    "\n",
    "***Training Process***\n",
    "\n",
    "The training process is an iterative procedure wherein the network adjusts weights and biases to minimize error (or loss) between its predictions and actual labels (ground truth). An optimization algorithm such as gradient descent is implemented to this end. During gradient descent, the gradient of the loss function with respect to the weights is computed. then, the weights are updated in the direction that reduces prediction error. The weight update rule for gradient descent is expressed mathematically as: \n",
    "\n",
    "$$\n",
    "\\text{New weight} = \\text{Old weight} - \\eta \\cdot \\frac{\\partial L}{\\partial W}\n",
    "$$ \n",
    "\n",
    "Where: \n",
    "- $ \\eta $ (eta): the learning rate, a hyperparameter that determines the size of steps taken during training \n",
    "- $L$: the loss function, which measures the error between predictions and actual values \n",
    "- $W$: the weight matrix to be continuously updated \n",
    "- $\\frac{\\partial L}{\\partial W}$: the gradient of the loss function with respect to the weight matrix \n",
    "\n",
    "The gradient of the loss function $ L $ with respect to the weight matrix $ W $ is expressed by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "\n",
    "***Activation Function Selection***\n",
    "\n",
    "An activation function is applied to the weighted sum of inputs plus bias at each layer, introducing nonlinearity to the model. It plays a critical role by determining the output of each neuron based on its input, enabling ANNs to model complex relationships. \n",
    "\n",
    "Examples include: \n",
    "\n",
    "1. Sigmoid: outputs values between 0 and 1. \n",
    "2. Rectified linear unit (ReLU): popular in the hidden layers of deep networks, ReLU \"rectifies\" negative values to 0 while leaving positive values unchanged. \n",
    "3. Softmax: commonly applied in the output layer for multi-class classification problems. Outputs a normalized vector of probabilities between 0 and 1 whose sum equals 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Implementation in Python***\n",
    "\n",
    "We'll be setting up our model manually to demonstrate its inner mechanics. This approach helps us understand the fundamental concepts behind neural networks, such as how layers, weights, and activation functions work together to make predictions.\n",
    "\n",
    "In practice, it can be convenient to use one of several powerful libraries and frameworks for neural network implementation. These offer pre-built models with various architectures and optimization routines to streamline model development and training by removing the need for writing low-level code from scratch. In this notebook, we will utilize TensorFlow, an open-source framework developed by Google. It provides a comprehensive ecosystem for building and deploying machine learning models. TensorFlow is widely used because it offers:\n",
    "\n",
    "- Ease of Use: High-level APIs like Keras make it easy to build and train models.\n",
    "- Performance: Optimized for both CPU and GPU, allowing for efficient training and inference.\n",
    "- Flexibility: Supports a wide range of machine learning tasks, from simple linear regression to complex deep learning models.\n",
    "- Community and Support: A large community of developers and extensive documentation.\n",
    "- Alternatives to TensorFlow\n",
    "\n",
    "Alternatives include: \n",
    "\n",
    "- PyTorch: Developed by Facebook, PyTorch is known for its dynamic computation graph and ease of use, especially in research settings.\n",
    "- Keras: Initially an independent project, Keras is now part of TensorFlow. It provides a high-level API for building neural networks.\n",
    "- MXNet: An open-source deep learning framework that is scalable and supports multiple languages.\n",
    "- Caffe: Developed by the Berkeley Vision and Learning Center (BVLC), Caffe is known for its speed and modularity.\n",
    "- Theano: One of the earliest deep learning libraries, Theano is now mostly used for educational purposes.\n",
    "\n",
    "Let's proceed with our setup for a manual implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Set model characteristics and hyperparameters\n",
    "\n",
    "input_layer_size = 3 # number of features\n",
    "hidden_layer_size = 4 # number of hidden units\n",
    "output_layer_size = 1 # number of classes\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Randomly initialize weights and biases\n",
    "w1 = np.random.randn(hidden_layer_size, input_layer_size + 1)\n",
    "b1 = np.random.randn(hidden_layer_size)\n",
    "w2 = np.random.randn(output_layer_size, hidden_layer_size + 1)\n",
    "b2 = np.random.randn(output_layer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Explanation***\n",
    "\n",
    "np.random.randn() generates a matrix of random numbers from a standard normal distribution ($\\mu$ = 0, $\\sigma$ = 1).\n",
    "\n",
    "Random initialization ensures that the network does not begin training with any pattern, thus optimizing learning and helping ensure that the model converges. \n",
    "\n",
    "The weight matrix w1 describes strength of connections between neurons in the input and hidden layer. Its dimensionality is (hidden_layer_size, inputer_layer_size + 1): the number of rows corresponds to the number of neurons in the hidden layer, while the number of columns corresponds to the number of neurons in the input layer + 1. This accounts for the bias term, an additional weight added at each node. This value helps the model better fit the data. \n",
    "\n",
    "The matrix w2 contains weights between the hidden and output layer. Dimensionality here is (output_layer_size, hidden_layer_size +1). The number of rows corresponds to the number of neurons in the output layer, while the number of columns corresponds to the number of neurons in the hidden layer plus one, which accounts for bias terms. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
